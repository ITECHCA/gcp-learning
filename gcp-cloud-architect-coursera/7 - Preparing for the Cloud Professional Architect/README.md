# Preparing for the Cloud Professional Architect

## Introduction

Exam is about the 4P's: Product, People, Process and Policy.

Review the certification study guide to know if you are ready to give a shot or not

To be best prepare, set yourself up in a Cloud Architect position. This is a test for practitioner, not for theorical knowledge

## About Google Cloud Certifications

Exam is about 2 hours. Cost $200

## Case studies
cf paper notes

## Preparing for design business

When answering a question, make sure you think through of all the different layers you 
learned in this course:
* Service definition
* Business Layer
* Data layer
* Network and presentation layer
* Scalability and recovery
* Security
* Capacity and cost
* Deploying and Monitoring

### Designing a solution infrastructure that meets business requirements

* Even the best design does work well for every requirement.
* Consider trade-off: Good (Build) / Inexpensive (Modify) / Fast (Buy)
    * What are the business priorities
    * What is existing in the client infrastructure 

Exam outline:
* Business use cases and product strategy: What does success look like qualitatively
* Cost optimization: A separate step after business logic criteria are met
* Supporting the application design: What procedures and processes will be needed
* Integration: Working with existing systems in parallel at first ? Are there parts that must remain on premises ?
* Movement of data: How and when will the data migrate to the new solution ?
* Tradeoffs: Priorities. Timing can be critical value
* Build, Buy, Modify: Control vs overhead vs time
* Success measurements: What does success look like quantitatively ?

## Prepare case study analysis

Things to consider in this case study:
* GCP projects generated by a reviewer and handled to a developer
* VPN access to avoid public Internet
    * HA Cloud VPN
* Active directory synchronization
    * Using Google Cloud Directory Synchronization
* IAM policy management
* Organizations and folder management

Things not said in the case, but also to consider
* Data protection, level of confidentiality
* Billing account

## Preparing for technical design

### Designing a solution infrastructure that meets technical requirements

Exam outline:
* High availability: What are the real criteria ? What are the measurable goals ?
    * Sometimes, a client just want to use the Cloud as a Failover solution
* Elasticity of the cloud resources: How do you want the solution to behave when busy ? How will the system behave when under attack ? How will it behave when traffic diminishes ?
    * After the failover solution, you can use cloud as an extra environment for extra load
* Scalabilty to meet growth requirements: Scaling for growth is different from autoscaling to cover temporary demand on non linearities. When it is time to add a node, upgrade a service, or switch services ?

If a requirement is not fully described - perhaps it is not important to the question being asked
   
This pattern of adoption is very common, that's why those concerns are colocated in an exam.

1. Begin Simply. Iterate.
    * Compose simple services. Build up to complexity. Iterate
2. Plan for failure
    * Avoid bottlenecks
    * Avoid SPOF
3. Measure stuff
    * Consider the situation: What are the limits ? How might the system fail ?
    
Tips: All business values are real time values

* In a stateful environment, it is important to keep the state. what solutions you use ?
* In a stateless environment, The microservices need coordination, and messaging systems

Common pattern:
* Separating backend/frontend
* Stateful vs stateless
* Access first via DNS and HTTPS Load balancer

### Designing Network, Storage and Compute Resources

Exam outilne
* Integration with on premises/multi-cloud environments: 
    * When to use gsutil, gsutil rsync, and Storage Transfer Service
* Cloud native networking (VPC, peering, firewalls, container networking)
    * Understand all the networking serviecs and how to connect for throughput, security, billing and so forth
* Identification of data processing pipeline matching data characteristics to storage systems
    * Velocity (how frequent), Volume (How much), Variety (Format, structure), Volatility (how often does it change) - which services match ?
* Data flow diagrams
    * Very helpful to know where the data will travel through the solution and to look for bottlenecks and flow monitoring and control
* Storage system structure (e.g Object file, RDBMS, NoSQL, NewSQL)
    * ACID - What is eventual consistency ? BASE
    * Structure requirements ?
* Mapping compute needs to platform products
    * Scale, Capacity, control/overhead
    
Tips: Identify the bottlenecks

If a requirement is: I want to handle multiple users, where is/Are the bottlenecks ?
* Is it bandwidth, Throughput ? Latency ? Gigabytes ? Queries per second ?
* Where will the application hit its limits ?

Be able to picture a data diagram flows. Don't assume them symmetrical, or having the same capacities.

Transactions qualities ?
* Consistency: ACID
* Availability: BASE: Basically Available, Soft State, Eventual consistency
* Partition tolerance

### Creating a migration plan

Exam outline
* Integrating solution with existing systems: Parallel ?
* Migrating systems and data to support the solution
    * How much, how to synchronize in both directions ? Where is the source of data truth ?
* Licensing mapping
    * Datacenter licence to cloud licence - may have different licence or requirements for licenced software
* Network and management planning
    * How will on premises connect to the cloud solution
* Testing and proof of concept
    * Different ways to divide a test audience: random, group, phased, test
    
Tips: Where is the source of truth ? How will actions be synchronized ?

* Planning: Technical solution
* Doing: Practical considerations

Exam outline
* Cloud and technology improvements
    * When the technology improves, how will the solution embrace the improvements ?
* Business needs evolution
    * When the business changes, how will the solution change ? Most solutions don't just need to work once, they need to become self-sustaining ?
* Evangelism and advocacy
    * Who are the "gatekeepers" and what do they need to know to effectively do their jobs ?

Every 10 x in growth, something break. Always changing, so reconsider the first criteria. They may no longer be true.

## Preparing for managing

Exam outline
* Extending on-premises (hybrid networking)
    * VPN. Interconnect
* Extending to a multi-cloud environment
    * Interoperation interface. stackDriver (GCP + AWS)
* Security
    * Discusses in "Designing for security and compliance"
* Data protection
    * Encryption, access, CsEK, KMS

Interconnection:
* Layer 3: Direct peering or Carrier Peering
* Layer 2: Dedicated interconnect or Partner interconnect

Security: Raise the cost of violating the security above the value of the data
* Edge protections
    * VM access/IP addresses
    * API access / endpoints
* Network protections
    * Firewall is the first line of defense
* Infrastructure protections
    * Isolation and controlled sharing of resources
    * Server-side encryption and key management
    
## Case study 2

* Terraform for IaC and deployment manager
* IAM strategy - Least privilege
* Service accounts
* Specific: Utilize source control options (GitHub Enterprise ?)

## Preparing for Data Processing

### Configuring Individual Storage systems

Exam outline
* Data storage allocation
    * Where will the Data be located ? Resiliency ? Charges ?
* Data processing / compute provisioning
    * Which platform ? What Capacity ?
* Security and access management 
    * CSEK and Cloud storage for example
* Network configuration for data transfer and latency
    * Location makes a difference in egress charges and round-trip time
* Data retention and data lifecycle
    * What is the Nealine and Coldline policy ? When to delete data ?
* Data growth management
    * When do you need a different way to organize the data ? 
    * How much/how big will the current design support ?

You have to know the services, and differences between then. For instance, which one is a serverless service, or fully managed 
is not something you want to do in the exam

BigQuery: Inexpensive datastore for tabular data.
    * Storage and analysis => Data warehouse solution

* Datastore: 1MB/entity
* BigTable: 10Mb/cell, 100MB/row
* Cloud storage: 5TB / object
* Cloud SQL: determined by DBEngine
* Cloud spanner: 10.240 MB / row
* BigQuery: 10MB/row

### Data Transfer

### Cloud storage

Review the different classes of the Cloud Storage, and know the pros and cons:
* Multi-regional: High availability
* Regional: Any duration. Lower access fees, Higher storage cost
* Nearline: 30 days storage minimum, higher access fees, lower storage cost
* Coldline: 365 days storage minimum, higher access fees, lower storage cost

Tips: Avoid the use of sensitive information as part of a bucket or object names

### BigTable

* Colossus: like HDFS, but with higher durability.
* Tablets: Like regions in HBase
* Table: Store the data
* Rowkey: The ONLY unique index of a table
    * It is not possible to create other indexes
* Organize data in family
* To optimize it, think carefully on how you want to access your data, what are your use cases
* Simple, so makes it fast, but can have limitation (like only one index, the rowKey)

### Data processing to Machine Learning

* Migrate from datacenter processing, to cloud center processing
* Then move to Machine learning
* Pre-trained ML
    * Cloud Vision API
    * Cloud speech to text
    * Cloud text to speech
    * Cloud Translation API
    * Cloud Natural language API
    * Cloud Visio Intelligence API
    * DialogFlow Enterprise Edition
* Custom ML
    * Cloud machine Learning Engine
    * Cloud AutoML
    * Cloud TPU
    
## Preparing for Compute
Exam outline
* Compute system provisioning
    * Speed, Throughput, capacity, burstiness ?
    * Turn off when not in use ?
* Compute volatility configuration (preemtible vs standard)
    * Stateful vs stateless
    * Tolerance for lost data/state
* Network configuration for compute nodes
    * Firewall rules, load balancing, NAT, VPN
* Orchestration technology configuration (e.g chef, puppet, Ansible, Terraform)
    * Deployment manager ... others
    * The value of self-documenting and automated orchestration of infrastructure
* Container orchestration (e.g Kubernetes)
    * The value of container-based development, portability, scalability, fault isolation, platform agnostic deployment, CI/CD development methods
    
Tips: Practice deciding which platform to use and know exactly why you would make that choice. It is an important infrastructure skill

### Microservices, Containers, Data procesing and IoT

Tips: 
* Microservices are not a panacea. They are not the best solution in every case
* Make sure you know where the edge cases are between technologies

* Generally, MicroServices are deployed using Cloud Functions or App engine

* Containers = Performance, Repeatability, Isolation and Portability
    * Does not manage the infrastructure or the OS layer
* VM: Does not manage the Infrastructure
* Bare metal: Manages everything

Pod: Kubernetes abstraction to represent an application

Tips: Make sure you understand the business goals of the exam question and not just the technical requirements

Know the difference:
* Managed service gives you visibility, but limited control
* Serverless completly hides all servers

Tips: Distinguish between data processing and computing cases. Sometimes, the solution is easier than it appears

IoT

The basic components if IoTCore with cloud Pub/Sub, to send data. The PubSub can trigger a Cloud Functions for analysis, sending configuration back to devices, or a Cloud Dataflow, for data streaming processing

Tips: Do you know how each of Cloud Functions, Cloud Pub/Sub and Cloud DataFlow contribute to the IoT Solution ?


### Designing for security
Exam outline
* Identity and Access Management (IAM): Use groups
* Data security (key management, encryption): Principle of least privilege
* Resource hierarchy (organizations, folder, projects): Separation of responsabilities to match organization structure
* Penetration testing: Shape the scope and the objectives of the test
* Separation of duties: Isolate key roles. Always have an alternate/secondary who can take over if needed. Service accounts
* Security controls: How are you monitoring security ? What logs and reports are available to you ? What responses will you take ?
* Management customer - supplied encryption keys with Cloud KMS
    * Key management, key rotation, standards and policy compliance

Tips: Can you apply the principle of least privilege in case examples ?

### Designing for Legal Compliance
Exam outline
* Legislation (eg health Insurance Portability and Accountability Act, Children Online Privacy PRotection)
    * HIPAA, GPDR
    * PII: Personally Identifiable Information
* audits (including logs): Do your policies conform to audit requirements
* Certification (e.g information technologye infrastructure library framework)
    * Firewall IAM Keys, CSEK
    * Make sure that growth doesn't break compliance
    
Google provides the security to a certain point. After that, it is up to us.
* Avoid external IP addresses, secure connection with SSL, Key management / rotation
* Separate VMs with VPC network
* Use 3rd part lib, like Cloud Armor

## Case study 4

CICD pipeline:
* Cloud Build connected to a Cloud Repository
* Container registry to store docker images
* Spinnaker for GCP for CD with Canary Testing
* GKE for running environment
* LoadBalancing, Stackdriver, Cloud IAM + Service Accounts

## Preparing for Analyzing processes

Exam outline
* Software Development Lifecycle Plan (SDLC)
    * Choosing developer tools on GCP
* Continuous integration/continuous deployment
    * Kubernetes... Container builder
* Troubleshooting / post mortem analysis culture
    * Blame isn't root cause. Focus on systems - what you can change in your procedures, not who is at fault
* Testing and validation
    * Error budget vs perfection
* IT enterprise processes (ITIL)
    * Information Technology Infrastructure Library defines processes, skills, and handoff of responsabilities
* Business continuity and disaster recovery
    * What's your D.R story ? How do you know it works, if you don't test it ? Tradeoffs, What are your SLAs ? 
    
    
### Analyzing and defining business processes

Exam outline
* Skateholder management (influencing and facilitation)
    * Who are the gatekeepers and what roles do they play ?
    * How can you enable them to do their jobs ?
* Change management
    * Quality is a process, not a product. Change is inevitable in cloud. Adapt process to be continuous
* Team assessment / skills readiness
    * Do you need a playbook ? Rehearsals ?
* Decision-making process
    * How long do you have to make which decisions ?
* customer success management 
    * How do you know that what you delivered is still meeting your customer/client's needs ?
    * When is it time for version +1 or version 2.0 ?
* Cost optimization / resource optimization (Capex / Opex)
    * What are the financial goals and how are they measured and reported
    
### Developing procedures to test resilience

You can't test everything. 

Is this resilient ?
* Is this a good design for handling failover ?
* Will it work ?
* Why or why not ?
* What procedures would you implement ?

Auto-scaling follows the law of diminishings returns

CPU utilization is rarely a good measure of customer experience. Not the good measure for you to ensure a good autoscaling

Cloud VPN does not have a real SLA, even if it has a 99.9% SLA, it doesn't mean the internet is...
* So no real SLAs

* Up to 10Gbps, partner interconnect is the most cost effective solution. 
* Above that, dedicated interconnect should be the solution 

### Case study 5

## Preparing for Advising

Exam outline
* Application development: Kubernetes, App Engine, Containers
* API Best practices
    * Be familiar with the Google Cloud SDK and how it works for automating infrastructure.
    * Be familiar with ML APIs such as Cloud Natural Language API
* Testing frameworks (load/unit/integration)
    * Be familiar with the different kind of testing: black box/white box, unit, integration
    * Be able to differentiate them
* Data and system migration tooling
    * Especially be familiar with the features for migrating or synchronizing data between on-premises or non-GCP cloud and GCP

* Do a checklist to know where you're at, and what you still need to focus on.
* Release management needs to be fully automatized (as much as possible)

Capacity planning for launch
* Test first
* Work through issues before serving user traffic
    * Identify bottlenecks
* Slow, staged, iterative
    * Dark launches
    * Use invitations to stage launch
    
## Preparing for reliability

### Ensuring solution and operations reliability

Tips: Know this item in order to have a first understanding of SRE
* Monitoring
* Incident response
* Post mortem / Root cause analysis
* Testing and Release procedures
* Capacity planning
* Development
* Product

Measures: 
* Metrics, Reports, SLIs
* Objectives, goals, watermarks, limits

Alert on customer pain. Alerting for the right reason.
* You can only monitor the interactions
* Black vs white box monitoring
    * Black box: Not suppose to know how the backend service works. You only focus on the UI
        * Validating user experience
    * White box: Use that knowledge of the system to define the test
        * Test focus on inner parts, like performance...

* Know how to use trace and debug. How do you use them together.

* Easy buttons are tools that automate common actions.

* Dashboard: a display for the system metrics.
* Alerts: alert when a given value has match a specific condition
* Incident response: How you response to a problem
    * Easy button, playbook

"Hope is not a strategy". Strategies for dealing with a failure
* Obviation: Design a system in which a particular type of failure is impossible
* Prevention: Take steps to ensure that a possible does not occur
* Detection and migration: Detect a failure before or as it is happening, and take teps to reduce or eliminate the effects of it
* Graceful degradation: Instead of failing completly, handle stress and return to full service when the issue passes
* Repair: Fix the problems. Hopefully, it won't come back, at least not in the same way
* Recover: Allow the problem to occur and get service back as quickly as possible. Measure indicators of recovery and work to improve them


Human processes:
* Evaluation
* Decisions
* Actions

## Practice case study analysis
### Case study 6

Microservices use cases. 
* Indenpency between services, resilience (one down does not crash the entire backoffice)
* Rolling update / easy rollback
* Custom Metrics for usage

My solution:
* App engine (or kubernetes engine, or Cloud runs) for microservices
    * they don't have any constraints regarding the network
* To increase independance: Cloud Pub/Sub, making sure events are published
    * If a service is down, the rest of the system keeps publishing, and then when the service is up again, it can pull the events
* StackDriver for monitoring applications, create dashboard and metrics
* Easy rolling update, and rollback if needed
* no constraints about database, so any can be used given the microservices

## Challenge labs

* Create a Source repositories to upload your files into
* Create a Cloud Build to generate the image in the Cloud Registry
    * Update the build definition to build using the tag `v2 `
    * Or use the command line
* Update the kubernetes deployment and scale it to 2 replicas
```
gcloud container clusters get-credentials echo-cluster --zone=us-central1-a
kubectl create deployment echo-web --image=gcr.io/qwiklabs-resources/echo-app:v1
kubectl expose deployment echo-web --type=LoadBalancer --port 80 --target-port 8000

# Get the folder from the bucket
gsutil cp gs://qwiklabs-gcp-00-708c300b3c57/echo-web-v2.tar.gz .
# Untar into a new folder
mkdir challenge-labs && tar -C challenge-labs -xvf echo-web-v2.tar.gz

cd challenge-labs
# Build the echo-app:v2 tag into container registry using Cloud Build
gcloud builds submit --tag gcr.io/qwiklabs-gcp-00-708c300b3c57/echo-app:v2 .
# Rolling update of the deployment
kubectl set image deployments/echo-web echo-app=gcr.io/qwiklabs-gcp-00-708c300b3c57/echo-app:v2
# Scale it to 2 replicas
kubectl scale deployments/echo-web --replicas=2
```

### Next steps

